{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meliksahb/Design-of-Intelligent-Machines-ME536-/blob/main/CLSTR_e2738425.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Melikşah Beşir e2738425**"
      ],
      "metadata": {
        "id": "htBKVXqf3tDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ME 536**\n",
        "\n",
        "### Find and count characters and their elements in strings\n",
        "Check if you can see the hidden ***SVD flavor*** somewhere in the requirements!"
      ],
      "metadata": {
        "id": "IO8uCGaWqUTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# only importing from following libraries are allowed. You can add more imports from them, but no new libraries here\n",
        "from skimage import io\n",
        "from skimage.filters import threshold_otsu as otsu\n",
        "import numpy as np\n",
        "from scipy.linalg import orth\n",
        "from numpy.linalg import matrix_rank as rank\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering # just for demo purposes, you can import more if needed...\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from scipy.signal import find_peaks\n",
        "from scipy.ndimage import label\n",
        "\n",
        "# also import the matrix printing function\n",
        "!rm bug_numpy_utils.py 2> dump.me\n",
        "!wget https://raw.githubusercontent.com/bugrakoku/bug_python_utils/main/bug_numpy_utils.py\n",
        "from bug_numpy_utils import CData as CMe\n",
        "from bug_numpy_utils import GenerateDataforImage as GenImMat\n",
        "from bug_numpy_utils import text2mat"
      ],
      "metadata": {
        "id": "WVNiukn8wyxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to Basics: Assignment has not started yet!\n",
        "This is the warm up"
      ],
      "metadata": {
        "id": "zZEa-nLiEhqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate and plot reference text\n",
        "Generate data matrix from a string.\n",
        "\n",
        "Columns of this matrix are data points, which when plotted is read as the given string.  \n",
        "\n",
        "Using pyplot display the data points to make sure that they are readable.  \n",
        "\n",
        "The problem is given in 2D below, play with the ```NoiseLevel``` and observe how data points mere into each other."
      ],
      "metadata": {
        "id": "Lm7YOvsyEgIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this is a support function to see the result of clustering better\n",
        "\n",
        "def ColorizeChars(M, Mnum = [], Title='some string', figSize = (9, 3),  aspectR = 2):\n",
        "    Indicies = np.hstack((np.array([0]), np.cumsum(Mnum)))\n",
        "    fig, ax = plt.subplots(figsize= figSize) # Increased figure size\n",
        "    # absence of Mnum is that we do not want to colorize the plot\n",
        "    if Mnum is None or Mnum is [] or len(Mnum) == 0:\n",
        "        ax.plot(M[0,:],M[1,:], '*')\n",
        "    else:\n",
        "        for i, uLim in enumerate(Indicies):\n",
        "            if i < len(Indicies)-1:\n",
        "                X = M[0, uLim:Indicies[i+1]]\n",
        "                Y = M[1, uLim:Indicies[i+1]]\n",
        "                #plt.plot(X,Y, '*')\n",
        "                ax.plot(X,Y, '*')\n",
        "    ax.set_aspect(aspectR) # Set the aspect ratio to aspectR:1\n",
        "    ax.set_title(Title)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "6Gz5uEez-D3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "S1 = 'hello clustering'\n",
        "T1, T1num = text2mat(S1)\n",
        "\n",
        "# just that we get to understand ```text2mat``` function let's print the output of the function\n",
        "print(f'Shape of T1 = {T1.shape}, where letters of \"{S1}\" has {T1num} data points in each corresponding letter')\n",
        "\n",
        "NoiseLevel = 0.0\n",
        "\n",
        "T1 += NoiseLevel * np.random.randn(*T1.shape)\n",
        "\n",
        "\n",
        "ColorizeChars(T1) # just plot the data as a single chunk\n",
        "ColorizeChars(T1, T1num, Title='clusters marked', figSize=(7,2), aspectR=4) # call this function anyway you like"
      ],
      "metadata": {
        "id": "BVEyPxKC8sSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WARNING: Testing conditions is not Vanilla\n",
        "Note that data matrix that will be sent might slightly be manipulated after it is generated with ```text2mat```, way beyond adding noise.  \n",
        "Check out the following to give you an idea.\n",
        "\n",
        "Note that when data matrix is shuffled, color printing makes no sense, because the columns are no more sorted, hence values returned by text2mat does not make sense.  \n"
      ],
      "metadata": {
        "id": "NQf2maVY_CBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 1\n",
        "Stest = 'ahanda boyle'\n",
        "Ttest, Tnum = text2mat(Stest)\n",
        "noiseLevel = 0.9\n",
        "\n",
        "#'''\n",
        "# original text\n",
        "V1 = Ttest[0:2, :]\n",
        "ColorizeChars(V1, Title='Original text')\n",
        "#'''\n",
        "\n",
        "#'''\n",
        "# contaminated ... play with noise levels\n",
        "V2 = V1 + noiseLevel * np.random.randn(*V1.shape)\n",
        "ColorizeChars(V2, Tnum, Title=f'Noisy level = {noiseLevel}')\n",
        "#'''\n",
        "\n",
        "#'''\n",
        "# extended or shrung along X-Y axis\n",
        "V3 = np.copy(V1)\n",
        "Xscale = 0.4\n",
        "Yscale = 1.2\n",
        "V3[0,:] *= Xscale\n",
        "V3[1,:] *= Yscale\n",
        "ColorizeChars(V3, Tnum, Title=f'Scaled along X and Y by =[{Xscale}, {Yscale} ]')\n",
        "#'''\n",
        "\n",
        "#'''\n",
        "V4 = np.copy(V3)\n",
        "V4 = V4[:, np.random.permutation(V4.shape[1])]\n",
        "ColorizeChars(V4, Tnum, Title='columns are shuffled, this is where fun begins')\n",
        "#'''\n",
        "\n",
        "#'''\n",
        "V5 = orth(np.random.rand(2,2)) @ V2\n",
        "ColorizeChars(V5, Tnum, Title='rotated randomly and noisy,  more fun :) ', aspectR=1)\n",
        "#'''\n",
        "\n",
        "\n",
        "#'''\n",
        "V6 = V5[:, np.random.permutation(V5.shape[1])]\n",
        "ColorizeChars(V6, Tnum, Title='shuffled and rotated and contaminated, almost the peak of fun', figSize=(7,7))\n",
        "# v = [V1, V2, V3, V4, V5, V6]\n",
        "#\n",
        "#'''"
      ],
      "metadata": {
        "id": "jGelzNgS_alB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment starts here\n",
        "Read the following cells carefully and respond by filling in the code and text cells. Your explanations brief yet clear.\n",
        "\n",
        "This assignment will hopefully make you better in clustering simple cases :)  \n",
        "For harder cases, we will talk about artificial neural networks..."
      ],
      "metadata": {
        "id": "cjzSCyqtS0oI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's start with a show case\n",
        "Note that ```text2mat``` function returns a data matrix and a list.  \n",
        "Also note that the data matrix is sorted, so that the first groups of points belong to the first letter, second group to the second letter and so on.  \n",
        "The list contains the number of points in each groups that correspond to the letters in the text that is sent to the function.\n",
        "\n",
        "So your objective indeed is to recover clusters, sort them, so that when we print, it plot them using what is returned from your function, it plots properly.  \n",
        "\n",
        "However, note that your sorting algorithm is not necessarily expected to find the order of clusters (i.e. letters), it is possible that you read the string from the end to beginning. By running the following you will see that the text might even be mirrorred. Under any circumstance, you should be able to find the alignment of the text, it does not matter wheter it is backwards or mirrorred, cluster it and return the sorted matrix along with the number of elements in each cluster, similar to what ```text2mat``` does. When we plot it using ```ColorizeChars``` it should look meaningful.\n",
        "\n",
        "In other words, your element count list should either be similar to what text2sum returns or to the inverse of the list.  \n",
        "\n",
        "Run the following cell for different noise levels and observe the changes.\n"
      ],
      "metadata": {
        "id": "Qpz4tsBxrlr8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering time:\n",
        "Using any approach you like sort points in the given data matrix.\n",
        "You can use hence import other sub-libraries in already imported libraries above.  \n",
        "No new libraries...\n",
        "\n",
        "Objective is to see if you can find letters indiviually.  \n",
        "In other words, after we shuffle everything, objective is to check if you can"
      ],
      "metadata": {
        "id": "EK-0g7kFtuPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implemet the sorting function: i.e. ```SortPoints()```\n",
        "If you would like to seperate this function into smaller other functions, write them in the support functions cell"
      ],
      "metadata": {
        "id": "zjTrGucYwOn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# support functions goes in here so that I run them before SortPoints()\n",
        "# write as many as needed\n",
        "# GPT Prompt: Is there any way to Distribution-based Clustering, Connectivity-based Clustering, and Density-based Clustering working together?\n",
        "# After created first code: This code create 2 cluster for 2 word. We need clusters for each letter\n",
        "# I edited code and subtract a few methods.\n",
        "def project_x_and_split(points, gap_threshold=10):\n",
        "    \"\"\"\n",
        "    Project points onto the x-axis, find gaps, and split into regions.\n",
        "    \"\"\"\n",
        "    # Project points onto the x-axis\n",
        "    x_coords = points[:, 0]\n",
        "    hist, bins = np.histogram(x_coords, bins=100)\n",
        "\n",
        "    # Find peaks and gaps in the histogram\n",
        "    gaps = np.diff(hist)\n",
        "    split_indices = np.where(gaps < -gap_threshold)[0] + 1\n",
        "    split_points = bins[split_indices]\n",
        "\n",
        "    # Split points based on the detected gaps\n",
        "    regions = []\n",
        "    start = points[:, 0].min()\n",
        "    for split in split_points:\n",
        "        mask = (points[:, 0] >= start) & (points[:, 0] < split)\n",
        "        if np.sum(mask) > 0:\n",
        "            regions.append(points[mask])\n",
        "        start = split\n",
        "    mask = points[:, 0] >= start\n",
        "    if np.sum(mask) > 0:\n",
        "        regions.append(points[mask])\n",
        "\n",
        "    return regions\n",
        "\n",
        "def dbscan_per_region(regions, eps=3, min_samples=5):\n",
        "    \"\"\"\n",
        "    Apply DBSCAN on each region to separate individual letters.\n",
        "    \"\"\"\n",
        "    all_clusters = []\n",
        "    for region in regions:\n",
        "        if len(region) < 10:\n",
        "            continue\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples).fit(region)\n",
        "        labels = dbscan.labels_\n",
        "        for label in np.unique(labels):\n",
        "            if label != -1:\n",
        "                all_clusters.append(region[labels == label])\n",
        "    return all_clusters\n",
        "def merge_close_clusters(clusters, dist_threshold=5):\n",
        "    \"\"\"\n",
        "    Merge clusters that are close in space (for vertically disconnected parts).\n",
        "    \"\"\"\n",
        "    merged = []\n",
        "    used = set()\n",
        "\n",
        "    for i, c1 in enumerate(clusters):\n",
        "        if i in used:\n",
        "            continue\n",
        "        combined = c1\n",
        "        for j, c2 in enumerate(clusters):\n",
        "            if j in used or i == j:\n",
        "                continue\n",
        "            distance = np.linalg.norm(np.mean(c1, axis=0) - np.mean(c2, axis=0))\n",
        "            if distance < dist_threshold:\n",
        "                combined = np.vstack((combined, c2))\n",
        "                used.add(j)\n",
        "        merged.append(combined)\n",
        "        used.add(i)\n",
        "    return merged\n"
      ],
      "metadata": {
        "id": "eAXHrvYs05V4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SortPoints(M, K = -1):\n",
        "    '''\n",
        "    Columns of M are the data points\n",
        "    K is the number of clusters, if K = -1, this function is to find the number of clusters!\n",
        "\n",
        "    returns Ms, MsNum\n",
        "    Ms: the column sorted version of M, similar to what is given by text2mat before shuffle\n",
        "    MsNum: element count list --> the number of points in each cluster, similar to what text2mat returns\n",
        "    Note that len(MsNum) = number of letters in the original string\n",
        "    '''\n",
        "   # Transpose input data to (N x 2) format\n",
        "    points = M.T\n",
        "\n",
        "    # Step 1: Project and split points into regions based on x-axis gaps\n",
        "    regions = project_x_and_split(points, gap_threshold=50)\n",
        "\n",
        "    # Step 2: Apply DBSCAN within each region\n",
        "    initial_clusters = dbscan_per_region(regions, eps=2, min_samples=5)\n",
        "\n",
        "    # Step 3: Merge vertically disconnected parts of letters\n",
        "    refined_clusters = merge_close_clusters(initial_clusters, dist_threshold=8)\n",
        "\n",
        "    # Step 4: Sort clusters by horizontal position\n",
        "    sorted_clusters = sorted(refined_clusters, key=lambda x: np.mean(x[:, 0]))\n",
        "\n",
        "    # Step 5: Flatten and format the data\n",
        "    Ms = []\n",
        "    MsNum = []\n",
        "    for cluster in sorted_clusters:\n",
        "        Ms.extend(cluster)\n",
        "        MsNum.append(len(cluster))\n",
        "\n",
        "    # Convert back to required format\n",
        "    Ms = np.array(Ms).T\n",
        "    return Ms, MsNum"
      ],
      "metadata": {
        "id": "PrA9u9Y9uwyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's test the sorting function\n",
        "\n",
        "If you have implemented ```SortPints``` properly, following should work.\n",
        "I will only call ```SortPints``` to test your work.\n"
      ],
      "metadata": {
        "id": "w6NDDNC9wUiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's generate test data\n",
        "St = 'Test Data'\n",
        "Tt, Ttsum = text2mat(St)\n",
        "#in my case I will generate variations of Tt as I did above but for simplicty here I will stick with Tt\n",
        "\n",
        "#I will run one of the following\n",
        "R, Rsum = SortPoints(Tt) # case when k- number of clusters is given -\n",
        "# also check for fun what happens when k is given to be something different then the correct value\n",
        "for i in range(1, 6):\n",
        "    R, Rsum = SortPoints(v[i], len(Ttsum))\n",
        "    ColorizeChars(R, Rsum)\n",
        "\n",
        "# finally see the result\n",
        "\n"
      ],
      "metadata": {
        "id": "oZDbV7isumrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code starts by splitting the dataset horizontally to simplify the clustering process, ensuring that each region is small enough to contain one or two letters. Within each region, DBSCAN detects the actual letter clusters. Vertically disconnected parts are then merged using a proximity check to form complete letters. Finally, clusters are sorted for proper alignment.\n",
        "\n",
        "The points are projected onto the x-axis, and gaps in their distribution are identified using a histogram. These gaps split the dataset into smaller horizontal regions, each containing one or more letters. Within each segmented region, DBSCAN identifies dense clusters of points, corresponding to individual letters. DBSCAN works well because it can detect clusters without requiring the number of clusters in advance.\n",
        "\n",
        "Clusters that are horizontally close and vertically overlapping are merged into single clusters. This step ensures that disconnected parts of letters like T or M are unified.\n",
        "\n",
        "The final clusters are sorted based on their mean x-coordinate, ensuring the letters are aligned in left-to-right order.\n",
        "\n",
        "By combining projection-based splitting, DBSCAN clustering, and merging, the code effectively separates individual letters from noisy data."
      ],
      "metadata": {
        "id": "vKUuGg_MSM3T"
      }
    }
  ]
}